{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6UeqOiyEijl3",
   "metadata": {
    "id": "6UeqOiyEijl3"
   },
   "source": [
    "**MistralTechAgent Lightweight Technical Assistant Using Mistral-7B-Instruct**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fYuCNTAtiuD3",
   "metadata": {
    "id": "fYuCNTAtiuD3"
   },
   "source": [
    "# ğŸ“„ Description:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hmbRZinQivwt",
   "metadata": {
    "id": "hmbRZinQivwt"
   },
   "source": [
    "MistralTechAgent is a simple and efficient AI assistant powered by the Mistral-7B-Instruct model. It is designed to provide accurate and easy-to-understand answers to technical questions. This agent uses a lightweight instruction-tuned model that runs smoothly on limited hardware while delivering high-quality responses. It also supports a customizable prompt format for structured queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8qf1K5ycjU1r",
   "metadata": {
    "id": "8qf1K5ycjU1r"
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DhivyaBharathy-web/PraisonAI/blob/main/examples/cookbooks/MistralTechAgent.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6oVNaoxbfsCP",
   "metadata": {
    "id": "6oVNaoxbfsCP"
   },
   "source": [
    "# SETUP: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af21b3c7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "af21b3c7",
    "outputId": "3c900128-c9c1-4688-9f83-eaa2f9d2872e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xjXmh8sQf476",
   "metadata": {
    "id": "xjXmh8sQf476"
   },
   "source": [
    "# SETUP: Hugging Face Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8405c936",
   "metadata": {
    "id": "8405c936"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"Enter your hugging api key\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/content/hf_cache\"  # Optional: Faster repeat runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PfKEoHt3haFV",
   "metadata": {
    "id": "PfKEoHt3haFV"
   },
   "source": [
    "# YAML Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "iNfVRVN_hJOn",
   "metadata": {
    "id": "iNfVRVN_hJOn"
   },
   "outputs": [],
   "source": [
    "prompt_template = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"\"\"You are MistralTechAgent, an expert AI model assistant.\n",
    "You provide clear, concise, and technically correct answers.\"\"\",\n",
    "    \"user_prompt\": \"{{ user_input }}\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LJAKEnSQf8gq",
   "metadata": {
    "id": "LJAKEnSQf8gq"
   },
   "source": [
    "# Load the Model (Fallback Safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6a29e6a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545,
     "referenced_widgets": [
      "d84722f34b2847f29f5d72690fc4dac6",
      "cff68559d8ad46c190e7fe7f9403b9ee",
      "36abbc31f97041b6a7786ccc3214e6ff",
      "07d8084255bb465ab1eac565b9f8752f",
      "9deaf08dabdf4679a2a44bb76e9a90f7",
      "d07f025f4fe44db082fdf9ffc460ef63",
      "56f8056ba6964bef922831ba1d4b1c1f",
      "4f0419a3b0ab450aac64c1cd0ff12e5b",
      "fe77c87b1caa40ae841bd65f499fc83a",
      "83ba36ab3e5b455887f5d1fa5594ab78",
      "005ec31a3aed4dc9b6083a59a9868bbd",
      "15321d3071894e1c8948664d4e9066d2",
      "4b43fc4bbce14f98bf8fc253fa3ed054",
      "5d1662cd47b9477c84e77c29ce5ceeef",
      "0be07a3e7a094dd09672bea04df7bc80",
      "4b01774127fc4cf0b67baf13583ba8ed",
      "ef2f3f069ade44fa8000cfd560b618a9",
      "21b4e8cefdee4c5e8a2ba142d503fc87",
      "d426d2480422465eb8b625ebb584feb1",
      "8c4a7ec3cf8b462eaa5c9df357e216ee",
      "b80e114ea4e6475297b61f9693d231c1",
      "7c94da36524d402a81326a49a30cbbb5",
      "f005c11730fd499bb8f81aaf56c653d9",
      "c88dca8e5c164bd1a655adf7ced0e86a",
      "104aa6d812164a76be60cd7d39dd8cde",
      "f2729ef32b5d48bc8fb5a90f72d5ada2",
      "38fa0e67450e40ecaf0a35feacb26186",
      "0a74dde6fe1b45ef9c454dc23e4ccc9e",
      "238917a3bcc24b5fa68fe2c7edc9234b",
      "c6484ba3a50b4bec9086d8497b8c2692",
      "41f62ed77f0449c99362997b36a866ed",
      "ac54271d234f451eb793f7da28a8b802",
      "d1574df5392c4bb2a2a6dece0cc30679",
      "3a8da99cdb184f68b51d9706a867654b",
      "0e32a6e4428f4ef3afa9a487c1524f89",
      "b88cc5c73c734d15a4f8e073c8f3355a",
      "0dde4efd517d422cbc70f9379cf831f6",
      "4ffe478a8f654f008453fcb1e159b5c7",
      "053d6a80586e41ba88299b55570922ac",
      "f84040d89eb14f2ca2f3a88121c02182",
      "7536d676b440429cb157919373401266",
      "d2bada90caee4a37a6172ea6b046aec8",
      "0c84482a056a4dd89f0369fc0ce33a89",
      "648e7a00926845cd91344ce8cec2f8c7",
      "ffa6acc3f9ba41dc8a3b57716cd9e040",
      "a875f5aaea024290b2c347bdc9216b0c",
      "a60321d756a54737ba37053a43890129",
      "4aba7d0dd458437ab2d38d615fce9fc2",
      "7b2205697a1b41cdbe363c56345b8d01",
      "e2c0fd4a5436449cb3adca1f2204088e",
      "0e3fa8394aa749498e2898049fad0536",
      "fec704faaa5945b9917e730d2db8dd41",
      "6a117742183c4b7d8011df4b8a1a4a76",
      "60284b7ab8d845389419c7ac13d3cd3f",
      "2f36f3edcdd14cdbafa6217758ce99f6",
      "b9a0aeb8daf24fbd901a13cf728a7f31",
      "d769287686274598b83c6d4f3adf86eb",
      "6092a601fee7400187e85322d8c392d4",
      "3e397abc9c704ddb8b7bc8e0dcbee6bc",
      "ea3d99ef60e048fab9b9fb8c4c8c30b8",
      "8bb3bacb95b447a8b3c58edf8f61e787",
      "f586830e76dc42cd960b3f0b85103307",
      "6ad155c4df5d4f30a481e5041e45edd2",
      "31f8ebe87ae34e42ae0a78a6f9099191",
      "87bd98a893ca4a8eae7c8e1a12449b52",
      "3c3ccb3c9cbf4cacace8e24abb0a91ea",
      "1edd5f953fd04bc7b8d20b5ffceabb3c",
      "4df5649528874b209439c0c228dfabc0",
      "fd76767d90c14fa596ee606ce4ed04da",
      "259ecb69082644418e64abeb668ac24a",
      "31d294d5e1ec4b229cf6f84f4d5ae1b0",
      "24b5318baf2e4a45bda8fc93168b8a14",
      "503c7ac86036420ba69af49ac8b3b9c6",
      "13ad3a362cf341c1923abb4b3b1cd680",
      "1eb86202e6aa4fe39e8de1f0af54b794",
      "c24ffdfc0cd44a18b32803f93951029d",
      "99190548afb445b9bcc552ec16971a8f",
      "5b59bb8867b74cd28f342b5750232986",
      "cd1b87d6fdd94f92ad015506b2ba2c5e",
      "9a47c5378bce4a619409df36231ada16",
      "a9cac4aa7fc243238c17c3fa76d3fdfb",
      "d0da7d1ddbeb45deaeca52b1a8446ce1",
      "4c30d02b0b7947c4ad60bb822dac264e",
      "7a62762db5f84a6782663e36565f7cea",
      "64c0dff073c24b15b9a211112ccc2d47",
      "d6d5d90e878e4aa3a9d5c09d1ac8cd92",
      "c9fef86db14d4e8e9a127c1dd1edc729",
      "212f60982a33493497050c3d12c2363d",
      "05010758eed94155887ee707b3648ca0",
      "43f71ce03f874002afcabf2b64f946be",
      "5282b4041f064cbc890dcdaa3bbd6104",
      "07cb2d7c0c0f4bfd8ec7a3857a731390",
      "3a8973062f654466925d40e35fe5e063",
      "ded64c09b9b44838bc86e0e648c1c52b",
      "0eb43a6eb7aa484a9e07309479978586",
      "bbe209e71cad4c6d99a073bc900cf880",
      "8cb9fb71cbd64904bbad6c2c5f7d1ffe",
      "e8a986f524aa40339ab1f9fc5c92c0d4",
      "02a5f1682b5e4afb875f3542b4b26c74",
      "a7d0ecbe717f4427a99296ce2054e3fc",
      "ed0039d420fa47518e2bc4626a549c86",
      "0e14815b8e4d4fb2a39a97ca1e743a69",
      "022675075da14683b86d4e56d29e291e",
      "550b21f1e49e4cefa4a68d5b5e9bac01",
      "0b598550b4234654a8454d5f9c70b1e4",
      "880f0cf380fc4b499468db81b063a27e",
      "758ce764b5294a5ab0c97e439a172cb2",
      "410954f602984cd7bf39d2b12c5105c4",
      "8eab2746f98e4d57b148969f9af8b17f",
      "b622a9fb49fb43a6ae12004823b9b6e9",
      "53a30b0dc2034bf0aa28c853b9c0e270",
      "174544c42508419a9ea1dec2b5a48eec",
      "aaa735b347074f02b9b174724beabfce",
      "66fd00b5b01f46f1b362013d022ad437",
      "734c206c4bbb4cc08da0a061a0671baa",
      "5a185bcc5f814b1c829fa7a702a763a2",
      "c1928ef5ac3841a8a97c69b11ea8a6da",
      "2c318f2f01fc4271a890ec6b8238c9ad",
      "2d7ff096493b425eafb671f6d23de282",
      "493fd41d50bc4ed5aa33e58a50254edb",
      "5016341a1d0d4e59ad263aa81f81d5fd"
     ]
    },
    "id": "d6a29e6a",
    "outputId": "9417ca92-9348-427d-e8e1-aabf3e716dbd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Loading model: mistralai/Mistral-7B-Instruct-v0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py:902: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d84722f34b2847f29f5d72690fc4dac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15321d3071894e1c8948664d4e9066d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f005c11730fd499bb8f81aaf56c653d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a8da99cdb184f68b51d9706a867654b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa6acc3f9ba41dc8a3b57716cd9e040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a0aeb8daf24fbd901a13cf728a7f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1edd5f953fd04bc7b8d20b5ffceabb3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b59bb8867b74cd28f342b5750232986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05010758eed94155887ee707b3648ca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7d0ecbe717f4427a99296ce2054e3fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a30b0dc2034bf0aa28c853b9c0e270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "try:\n",
    "    print(f\"ğŸ”„ Loading model: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name, use_auth_token=os.environ[\"HF_TOKEN\"], trust_remote_code=True\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        use_auth_token=os.environ[\"HF_TOKEN\"],\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    model.eval()\n",
    "    print(\"âœ… Model loaded successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to load {model_name}\\nError: {e}\")\n",
    "    model_name = \"distilgpt2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GWj9n1cVg5xj",
   "metadata": {
    "id": "GWj9n1cVg5xj"
   },
   "source": [
    "# Define the MistralTechAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83492929",
   "metadata": {
    "id": "83492929"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MistralTechAgent:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def chat(self, prompt: str, max_new_tokens=256) -> str:\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,  # DETERMINISTIC output\n",
    "                temperature=1.0,\n",
    "                top_p=1.0,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        full_output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return full_output[len(prompt):].strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QZC6QzA4hB8m",
   "metadata": {
    "id": "QZC6QzA4hB8m"
   },
   "source": [
    "# Test the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83a3388e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83a3388e",
    "outputId": "ebfc13f5-bae9-4647-fc7c-78c386afa03c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Agent Response ===\n",
      "A language model is a type of artificial intelligence (AI) model that is designed to understand, interpret, and generate human language. It is a machine learning model that is trained on large amounts of text data to learn the patterns and structures of language, including grammar, vocabulary, and syntax.\n",
      "\n",
      "Language models can be used for a variety of tasks, such as language translation, sentiment analysis, text summarization, and speech recognition. They are often used in natural language processing (NLP) applications, where they help machines to understand and interact with humans in a more natural and intuitive way.\n",
      "\n",
      "There are several types of language models, including statistical language models, neural language models, and transformer-based language models. These models differ in their architecture, training methods, and performance on different NLP tasks.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "agent = MistralTechAgent(model, tokenizer)\n",
    "\n",
    "prompt = \"You are an AI agent helping with technical queries. Explain what a language model is.\"\n",
    "response = agent.chat(prompt)\n",
    "\n",
    "print(\"=== Agent Response ===\")\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
